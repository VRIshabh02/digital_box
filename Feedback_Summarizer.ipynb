{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq  \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punctuation = punctuation + '\\n'\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_summarizer(input_text, number_of_sentence):\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    word_frequencies = {}  \n",
    "    for word in nltk.word_tokenize(input_text):  \n",
    "        if word not in stopWords:\n",
    "            if word not in punctuation:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(input_text)\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(number_of_sentence, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)  \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = pd.read_csv(r\"dataset/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_links(input_text):\n",
    "    pettern  = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "    out_text = re.sub(pettern, ' ', input_text)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_repeated_characters(input_text):\n",
    "    pattern  = r'(.)\\1{2,}'\n",
    "    out_text = re.sub(pattern, r\"\\1\\1\", input_text)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    replace = r'[/(){}\\[\\]|@âÂ,;\\?\\'\\\"\\*…؟–’،!&\\+-:؛-]'\n",
    "    out_text = re.sub(replace, \" \", input_text)\n",
    "    words = nltk.word_tokenize(out_text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    out_text = ' '.join(words)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    out_text = [w for w in tokens if not w in stop_words]\n",
    "    out_text = ' '.join(out_text)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(input_text):\n",
    "    out_text = delete_links(input_text)\n",
    "    out_text = delete_repeated_characters(out_text)\n",
    "    out_text = clean_text(out_text)\n",
    "    out_text = delete_stopwords(out_text)\n",
    "    out_text = out_text.lower()\n",
    "    return out_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data['Processed Text'] = en_data['Text'].apply(text_prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_label_encoder = LabelEncoder()\n",
    "en_data['Category Encoded'] = en_label_encoder.fit_transform(en_data['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_X_train, en_X_test, en_y_train, en_y_test = train_test_split(en_data['Processed Text'], en_data['Category Encoded'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test, ngram_range):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, ngram_range))\n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_features_train, en_features_test = tfidf_features(en_X_train, en_X_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model_name):\n",
    "    if model_name == 'ridge_model':\n",
    "        model_name = RidgeClassifier()\n",
    "    elif model_name == 'random_forest_model':\n",
    "        model_name = RandomForestClassifier()\n",
    "    elif model_name == 'logistic_regression_model':\n",
    "        model_name = LogisticRegression()\n",
    "    elif model_name == 'kneighbors_model':\n",
    "        model_name = KNeighborsClassifier()\n",
    "    elif model_name == 'decision_tree_model':\n",
    "        model_name = DecisionTreeClassifier()\n",
    "    elif model_name == 'gaussian_nb_model':\n",
    "        model_name = GaussianNB()\n",
    "    model_name.fit(en_features_train.toarray(), en_y_train)\n",
    "    model_predictions = model_name.predict(en_features_test.toarray())\n",
    "    accuracy = accuracy_score(en_y_test, model_predictions)\n",
    "    print(f\"Accuracy on test for {model_name}: {accuracy}\")\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for RidgeClassifier(): 0.9865771812080537\n"
     ]
    }
   ],
   "source": [
    "en_ridge_model = fit_model('ridge_model')\n",
    "pickle.dump(en_ridge_model, open('en_ridge_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for RandomForestClassifier(): 0.9496644295302014\n"
     ]
    }
   ],
   "source": [
    "en_random_forest_model = fit_model('random_forest_model')\n",
    "pickle.dump(en_random_forest_model, open('en_random_forest_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for LogisticRegression(): 0.9932885906040269\n"
     ]
    }
   ],
   "source": [
    "en_logistic_regression_model = fit_model('logistic_regression_model')\n",
    "pickle.dump(en_logistic_regression_model, open('en_logistic_regression_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for KNeighborsClassifier(): 0.9664429530201343\n"
     ]
    }
   ],
   "source": [
    "en_kneighbors_model = fit_model('kneighbors_model')\n",
    "pickle.dump(en_kneighbors_model, open('en_kneighbors_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for DecisionTreeClassifier(): 0.8221476510067114\n"
     ]
    }
   ],
   "source": [
    "en_decision_tree_model = fit_model('decision_tree_model')\n",
    "pickle.dump(en_decision_tree_model, open('en_decision_tree_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for GaussianNB(): 0.9630872483221476\n"
     ]
    }
   ],
   "source": [
    "en_gaussian_nb_model = fit_model('gaussian_nb_model')\n",
    "pickle.dump(en_gaussian_nb_model, open('en_gaussian_nb_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    sentiment_score = (compound_score + 1) * 5 \n",
    "    return round(sentiment_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.959731543624161\n"
     ]
    }
   ],
   "source": [
    "en_data['Processed Text'] = en_data['Text'].apply(text_prepare)\n",
    "\n",
    "en_X_train, en_X_test = train_test_split(en_data['Processed Text'], test_size=0.2, random_state=0)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "en_features_train = vectorizer.fit_transform(en_X_train)\n",
    "en_features_test = vectorizer.transform(en_X_test)\n",
    "\n",
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(en_features_train, en_y_train)\n",
    "\n",
    "model_predictions = random_forest_model.predict(en_features_test)\n",
    "accuracy = accuracy_score(en_y_test, model_predictions)\n",
    "print(\"Accuracy on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text summary:\n",
      "It goes above and beyond industry standards by offering a comprehensive suite of features, including budgeting tools, spending analysis, and personalized financial insights tailored to each user's needs.\n",
      "Sentiment of test_1: 10\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"The app's exceptional security measures ensure users' peace of mind and protection against any cyber threats. It goes above and beyond industry standards by offering a comprehensive suite of features, including budgeting tools, spending analysis, and personalized financial insights tailored to each user's needs. Barclays' commitment to customer satisfaction is evident in its prompt and attentive customer support, providing users with the assistance they need whenever they have inquiries or concerns. Overall, Barclays has set a new benchmark for mobile banking apps, delighting users with its user-friendly interface, robust security, and unparalleled customer service.\"\n",
    "summary = nltk_summarizer(test_1, 1)\n",
    "print(\"Text summary:\")\n",
    "print(summary)\n",
    "test_1_sentiment = analyze_sentiment(test_1)\n",
    "print(\"Sentiment of test_1:\", test_1_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text summary 2:\n",
      "While it may not excel in any particular area, it provides a satisfactory user experience for basic banking needs.\n",
      "Sentiment of test_2: 5\n",
      "Text summary 3:\n",
      "The app's security measures are inadequate, leaving users vulnerable to cyber threats.\n",
      "Sentiment of test_3: 1\n"
     ]
    }
   ],
   "source": [
    "test_2 = \"The app offers a range of standard features commonly found in similar applications. While it may not excel in any particular area, it provides a satisfactory user experience for basic banking needs.\"\n",
    "summary_2 = nltk_summarizer(test_2, 1)\n",
    "print(\"Text summary 2:\")\n",
    "print(summary_2)\n",
    "test_2_sentiment = analyze_sentiment(test_2)\n",
    "print(\"Sentiment of test_2:\", test_2_sentiment)\n",
    "\n",
    "test_3 = \"The app's security measures are inadequate, leaving users vulnerable to cyber threats. It lacks essential features and suffers from frequent glitches and errors. Customer support is unresponsive, failing to address users' concerns in a timely manner.\"\n",
    "summary_3 = nltk_summarizer(test_3, 1)\n",
    "print(\"Text summary 3:\")\n",
    "print(summary_3)\n",
    "test_3_sentiment = analyze_sentiment(test_3)\n",
    "print(\"Sentiment of test_3:\", test_3_sentiment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
